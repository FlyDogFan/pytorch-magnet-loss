%%%% ijcai17.tex

\typeout{IJCAI-17 Instructions for Authors}

% These are the instructions for authors for IJCAI-17.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai17.sty is the style file for IJCAI-17 (same as ijcai07.sty).
\usepackage{ijcai17}

% Use the postscript times font!
\usepackage{times}


\usepackage{subfiles}
\usepackage{graphicx}
\graphicspath{{images/}{images/}}

% the following package is optional:
%\usepackage{latexsym}


\title{EECS 692: Replication Project Proposal\\
Metric Learning with Adaptive Density Discrimination}
\author{Mohamed El Banani}

\begin{document}

\maketitle

\section{Prompt}
Please submit a 1-2 page description of the class project you are proposing.
This should include the paper(s) you are replicating,

% a brief description of that paper (one to two paragraphs),
I will be replicating \texit{Metric Learning with Adaptive Density Discrimination}
by Oren Rippel, Manouhar Paluri, Piotr Dollar, and Lubomir Bourdev~\cite{rippel2016metric}.
In this work, the authors identify two challenge that they thing are impeding the progress in metric learning.
First, that while many of those methods try to learn a new space, they predefine the structure based on the original space.
This is an unnecessary constrain that may deteriorate from the quality of the learned space.
Second, the authors find that most previous methods operate on instances of data, rather than learned clusters.
This can also be problematic since instances of the same class may be at different parts of the space, and hence,
expecting instances of a class to always be close together may also be detrimental to the quality of the space. 

This ideas being proposed in this work are interesting as they deal with a subtle assumption that is commonly made in image classification; 
that one should represent a given concept by a single node/cluster.
This notion implies that we want to map all instances of bird or vehicle only to the same node. While this might work for classifying those classes,
it means that we are throwing a lot of information away to colapse semantically equivalenet, but visually dissimilar, instances onto the same class. 
Prime examples of this would be chickens and penguins, which look very disimillar, but are both birds. 
However, through assuming that there exists multiple subclusters within the data, we can allow the model to place semnatically differnet subclusters close to each other,
while meanining local discriminability. This learned space might have interesting properties allowing it to do things such as one-shot learning which I discuss below.  

% why it is interesting to replicate the work,
I chose to replicate this work because it presents a new approach to learning high-dimensional representations of visual data that leverages some interesting notions about how the space should be structured
and produces interesting qualitative results. However, some of the numbers that they report for their baselines seem a bit lower than I expected since I have worked on some of those datasets before.
Also, unlike many papers that have been produced recently from Facebook Research, the authors did not release their code.
As a result, I think this would be an interesting paper to replicate as it both introduces a new approach that requires a complex pipeline that integrates with multiple different algorithms
(K-mean clustering, targeted dataset sampling, and convolutional neural network training through backpropagation),
compares against an existing literature in metric learning that evaluates on the same methods,
and uses multiple evaluation metrics (classification accuracy, hierarchy recovery, attribute distribution).


% which aspect of the work you are replicating and your proposal for doing the replication.
While the authors do not provide their code, there have been previous attempts at replicating their code.~\footnote{https://github.com/pumpikano/tf-magnet-loss}
However, upon inspection of the code, it is clear that the replication was not completed, and has only been attempted at the MNIST dataset~\cite{lecun1998gradient} which is a very easy dataset, and one that has not been tackled in the original paper.
The authors report multiple experiments; Fine-grained classification, hierarchy recovery, and tSNE~\cite{maaten2008visualizing} visualization for attribute distribution.
For fine-grained classification, the authors also report their results on 4 different datasets.
I plan on re-implementing Magnet Loss (including the sampling and bookkeeping required to get it working), and testing it on the four datasets provided.
If I successfully replicate this aspect of their work, I will attempt to replicate the hierarchy recovery and visualization experiments.
For all replicated experiments, I will also have to replicate the reported experiments that they have implemented (softmax and triplet loss).

% % This will include whether you are doing a complete reimplementation or using the original implementation (and some evidence that it is available).
% What questions are you going to study in the replication, and possibly what extensions you hope to look into (and why).
During this replication, I am interested to find whether clustering losses can actually result in comparable performance to classification losses on large datasets. 
If I am able to replicate this work, I would like to extend it in multiple ways. First, I would like to compare against pairwise losses and see if the pattern claimed by the authors holds for it as well. Second, I would like to test the space and see if the space learned by this model can do one-shot learning better than spaces learned using other formulations (triplet loss, pairwise loss, classification). If the model can both do few-shot learning and demonstrates the attribute locality claimed by the authors, then it is possible to use such a model as a feature extractor within a more complex system, allowing for richer inference that includes determining which attributes a novel object may have and which classes its likely to be a member of. However, this leaves an important question; what is actually being represented within this space: visual or semantic similarity ? and which is more important? I am not sure how I can tackle such questions effectively now, however, I hope that through exploring such a model, potential questions might arise that can help address those issues. 



%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai17}

\end{document}
