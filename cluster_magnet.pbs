#PBS -N magnet_loss_pretrain         # Name of the job
#PBS -j oe                      # Join output and error in single file
#PBS -l walltime=3:00:00:00     # walltime requested
#PBS -l mem=16GB                # memory requested
#PBS -l nodes=1:pascal:ppn=16:gpus=4    # nodes requested: Processor per node: gpus requested
#PBS -V                         # Pass current environment variables to the script
#PBS -S /bin/bash               # Use /bin/bash as the shell
##########################################
#                                        #
#   Output some useful job information.  #
#                                        #
##########################################

IFS=$'\n' allocatedgpus=($(sed -e 's/.*-gpu\([0-9]\+\)/\1/' $PBS_GPUFILE))
echo I got allocated the ;following gpus
for g in "${allocatedgpus[@]}"; do
    echo "gpu: $g"
done

echo PBS: qsub is running on $PBS_O_HOST
echo PBS: originating queue is $PBS_O_QUEUE
echo PBS: executing queue is $PBS_QUEUE
echo PBS: working directory is $PBS_O_WORKDIR
echo PBS: execution mode is $PBS_ENVIRONMENT
echo PBS: job identifich_convAtter is $PBS_JOBID
echo PBS: job name is $PBS_JOBNAME
echo PBS: node file is $PBS_NODEFILE
echo PBS: current home directory is $PBS_O_HOME
echo PBS: PATH = $PBS_O_PATH
echo ------------------------------------------------------

cd /z/home/mbanani/projects/pytorch-magnet-loss
source set_env.sh

python train_magnet.py --dataset oxford --batch_size 256 --log_rate 10 --num_epochs 100 --experiment_name train_magnet_test --workers 16


echo Done
